{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "*Deep Q Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## CartPole\n",
    "\n",
    "今回扱う問題．ポールが倒れないようなカート操作ができるように学習させる．\n",
    "\n",
    "- [Cart Pole - Gymnasium Documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "以下の4つの状態を持つ\n",
    "\n",
    "| 状態 | 範囲 |\n",
    "| --- | --- |\n",
    "|カートの位置 | -4.8 ~ 4.8 |\n",
    "| カートの速度 | -Inf ~ Inf |\n",
    "| ポールの角度 | -24° ~ 24° |\n",
    "| ポールの角速度 | -Inf ~ Inf |\n",
    "\n",
    "また行動はカートを右に動かすか左に動かすかの2通り．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v1'\n",
    "env = gym.make(ENV, render_mode='rgb_array')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print('n_actions:', n_actions)\n",
    "print('n_states:', n_states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q関数\n",
    "\n",
    "Q関数となるニューラルネットワークを定義する．  \n",
    "- 入力：状態\n",
    "- 出力：全ての行動の価値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント\n",
    "\n",
    "Q関数を所持し，それを元に行動を決定できるエージェントをクラスとして実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.q = QNet(n_states, n_actions)\n",
    "        self.optim = optim.Adam(self.q.parameters(), lr=0.0001)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    def get_action(self, s, epsilon=0):\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        if torch.rand(1).item() < epsilon:\n",
    "            a = torch.randint(n_actions, (1,)).item()\n",
    "        else:\n",
    "            a = self.q(s).argmax().item()\n",
    "        return a\n",
    "\n",
    "    def update(self, s, a, r, next_s, gamma=0.99):\n",
    "        \"\"\"Q関数を更新する\"\"\"\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        next_s = torch.tensor(next_s, dtype=torch.float32)\n",
    "        q = self.q(s)[a]\n",
    "        target = r + gamma * self.q(next_s).max(-1).values.detach() # 正解\n",
    "        loss = self.loss_fn(q, target) # 損失\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward() # 逆伝播\n",
    "        self.optim.step() # パラメータ更新"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 報酬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "報酬は，ポールの角度の絶対値にマイナスをかけたものとする．  \n",
    "ポールの角度が0°に近いほど，報酬は大きくなる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(s):\n",
    "    cart_p, cart_v, pole_a, pole_v = s\n",
    "    r = -abs(pole_a)\n",
    "    return r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 描画\n",
    "\n",
    "ゲーム画面を描画する関数も実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env, lim=500, interval=50):\n",
    "    frames = []\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    for _ in range(lim):\n",
    "        a = agent.get_action(s)\n",
    "        s, _, done, _, _ = env.step(a)\n",
    "        frames.append(env.render())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def update(i):\n",
    "        im.set_array(frames[i])\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, frames=len(frames), interval=interval)\n",
    "    plt.close()\n",
    "    display(HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "学習を行う関数の実装．  \n",
    "行動決定→行動→状態遷移→報酬決定→Q関数更新 を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes, epsilon=0.2, gamma=0.99, lim=500):\n",
    "    for _ in tqdm(range(n_episodes)):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        for _ in range(lim):\n",
    "            a = agent.get_action(s, epsilon)\n",
    "            next_s, _, done, _, _ = env.step(a)\n",
    "            r = reward_func(next_s) if not done else -5\n",
    "            agent.update(s, a, r, next_s, gamma)\n",
    "            if done:\n",
    "                break\n",
    "            s = next_s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に学習させてみる．まずエージェント（Q関数）を初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期状態での性能はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(agent, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから学習させる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, agent, 1000, epsilon=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
