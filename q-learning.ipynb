{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q学習\n",
    "\n",
    "Q学習を実装してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## CartPole\n",
    "\n",
    "今回扱う問題．ポールが倒れないようなカート操作ができるように学習させる．\n",
    "\n",
    "- [Cart Pole - Gymnasium Documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "以下の4つの状態を持つ\n",
    "\n",
    "| 状態 | 範囲 |\n",
    "| --- | --- |\n",
    "|カートの位置 | -4.8 ~ 4.8 |\n",
    "| カートの速度 | -Inf ~ Inf |\n",
    "| ポールの角度 | -24° ~ 24° |\n",
    "| ポールの角速度 | -Inf ~ Inf |\n",
    "\n",
    "また行動はカートを右に動かすか左に動かすかの2通り．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v1'\n",
    "env = gym.make(ENV, render_mode='rgb_array')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print('n_actions:', n_actions)\n",
    "print('n_states:', n_states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Q学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態の離散化\n",
    "\n",
    "Q学習では状態を離散化する必要がある．  \n",
    "今回は全ての状態を6段階に分割する．つまり状態は$6^4=1296$通りになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIZITIZED = 6\n",
    "\n",
    "def make_bins(min, max, n=NUM_DIZITIZED):\n",
    "    return np.linspace(min, max, n+1)[1:-1]\n",
    "\n",
    "def digitize_state(state: np.ndarray) -> int:\n",
    "    \"\"\"状態を離散化する\"\"\"\n",
    "    cart_p, cart_v, pole_a, pole_v = state\n",
    "    digi_cart_p = np.digitize(cart_p, make_bins(-4.8, 4.8))\n",
    "    digi_cart_v = np.digitize(cart_v, make_bins(-4.8, 4.8))\n",
    "    digi_pole_a = np.digitize(pole_a, make_bins(-0.24, 0.24))\n",
    "    digi_pole_v = np.digitize(pole_v, make_bins(-4.8, 4.8))\n",
    "    digi = sum([NUM_DIZITIZED**i * d for i, d in enumerate([\n",
    "        digi_cart_p, digi_cart_v, digi_pole_a, digi_pole_v])])\n",
    "    return digi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント\n",
    "\n",
    "Q関数を所持し，それを元に行動を決定できるエージェントをクラスとして実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,):\n",
    "        self.q = np.random.randn(NUM_DIZITIZED**4, n_actions) # Q関数の初期化\n",
    "\n",
    "    def get_action(self, s, epsilon=0):\n",
    "        s = digitize_state(s) # 状態を離散化\n",
    "        if np.random.random() < epsilon: # ε-greedy法\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(self.q[s])\n",
    "        return a\n",
    "\n",
    "    def update(self, s, a, r, next_s, gamma, alpha):\n",
    "        \"\"\"Q関数を更新する\"\"\"\n",
    "        s = digitize_state(s)\n",
    "        next_s = digitize_state(next_s)\n",
    "        target = r + gamma * np.max(self.q[next_s])\n",
    "        self.q[s, a] = self.q[s, a] + alpha * (target - self.q[s, a])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 報酬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "報酬は，ポールの角度の絶対値にマイナスをかけたものとする．  \n",
    "ポールの角度が0°に近いほど，報酬は大きくなる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(s):\n",
    "    cart_p, cart_v, pole_a, pole_v = s\n",
    "    r = -abs(pole_a)\n",
    "    return r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 描画\n",
    "\n",
    "ゲーム画面を描画する関数も実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env, lim=500, interval=50):\n",
    "    frames = []\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    for _ in range(lim):\n",
    "        a = agent.get_action(s)\n",
    "        s, _, done, _, _ = env.step(a)\n",
    "        frames.append(env.render())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def update(i):\n",
    "        im.set_array(frames[i])\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, frames=len(frames), interval=interval)\n",
    "    plt.close()\n",
    "    display(HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "学習を行う関数の実装．  \n",
    "行動決定→行動→状態遷移→報酬決定→Q関数更新 を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes, epsilon=0.2, gamma=0.9, alpha=0.3):\n",
    "    for _ in tqdm(range(n_episodes)):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = agent.get_action(s, epsilon)\n",
    "            next_s, _, done, _, _ = env.step(a)\n",
    "            r = reward_func(next_s)\n",
    "            agent.update(s, a, r, next_s, gamma, alpha)\n",
    "            s = next_s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に学習させてみる．まずエージェント（Q関数）を初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期状態での性能はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(agent, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから学習させる．2000エピソード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, agent, 2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
